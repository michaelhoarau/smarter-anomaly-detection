{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1093add7",
   "metadata": {},
   "source": [
    "# **Smarter anomaly detection** - Data preparation step\n",
    "*Part 1 - Data preparation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b3061",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". smarter-anomaly-detection\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data are stored here\n",
    "|   ├── processed                        # Finalized datasets ready to be moved to Amazon S3\n",
    "|   └── raw                              # Immutable original data are stored here\n",
    "|\n",
    "└── notebooks/\n",
    "    ├── 1_data_preparation.ipynb         <<< THIS NOTEBOOK <<<\n",
    "    ├── 2_model_training.ipynb\n",
    "    └── 3_model_evaluation.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac342a",
   "metadata": {},
   "source": [
    "### Notebook configuration update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet --upgrade tqdm tsia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c704e8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691306e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthetic_config as config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tsia\n",
    "import zipfile\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc4576",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Let's first check if the bucket name is defined, if it exists and if we have access to it from this notebook. If this notebook does not have access to the S3 bucket, you will have to update its permission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4919301",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = os.path.join('..', 'data', 'raw')\n",
    "TMP_DATA = os.path.join('..', 'data', 'interim')\n",
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed')\n",
    "os.makedirs(RAW_DATA, exist_ok=True)\n",
    "os.makedirs(TMP_DATA, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA, exist_ok=True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "plt.rcParams['lines.linewidth'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c36ae2",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_fname = os.path.join(TMP_DATA, 'synthetic', 'sensors.csv')\n",
    "synth_df = pd.read_csv(synth_fname)\n",
    "synth_df['timestamp'] = pd.to_datetime(synth_df['timestamp'])\n",
    "synth_df = synth_df.set_index('timestamp')\n",
    "synth_df.index.name = 'Timestamp'\n",
    "synth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58f287",
   "metadata": {},
   "source": [
    "## Dataset visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc83a7a",
   "metadata": {},
   "source": [
    "This dataset contains some labels with failure and healing periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_df = synth_df[synth_df['machine_status'] == 'BROKEN'].copy()\n",
    "\n",
    "recovering_df = pd.DataFrame(index=synth_df.index, columns=['value'])\n",
    "recovering_df['value'] = 0.0\n",
    "recovering_index = synth_df[synth_df['machine_status'] == 'RECOVERING'].index\n",
    "recovering_df.loc[recovering_index, 'value'] = 1500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,6))\n",
    "plt.plot(synth_df['signal_05'], label='Signal 05')\n",
    "plt.plot(synth_df['signal_04'], label='Signal 04')\n",
    "plt.plot(synth_df['signal_19'], label='Signal 19')\n",
    "plt.plot(synth_df['signal_07'], label='Signal 07')\n",
    "plt.plot(synth_df['signal_00'], label='Signal 00')\n",
    "plt.scatter(broken_df.index, broken_df['signal_03'], marker='o', color=colors[1], s=100, edgecolor='#000000', alpha=0.8, zorder=3, label='Failure time')\n",
    "plt.fill_between(x=recovering_df.index, y1=recovering_df['value'], color=colors[2], alpha=0.4, label='Recovering period')\n",
    "\n",
    "plt.legend(loc='lower center', fontsize=10, ncol=7, bbox_to_anchor=(0.5, -0.15))\n",
    "plt.title('Synthetic sensor data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = list(synth_df.columns)\n",
    "num_cols = 2\n",
    "num_rows = len(tags_list) // num_cols + 1\n",
    "fig = plt.figure(figsize=(24, 5 * num_rows))\n",
    "\n",
    "for index, f in enumerate(tags_list):\n",
    "    ax = fig.add_subplot(num_rows, num_cols, index+1)\n",
    "    ax.plot(synth_df[f], color=colors[index % len(colors)])\n",
    "    ax.set_title(f)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in tags_list if f not in ['machine_status']]\n",
    "\n",
    "# Build a list of dataframes, one per feature:\n",
    "df_list = []\n",
    "for sensor in features:\n",
    "    tag_df = synth_df[[sensor]]\n",
    "    tag_df = tag_df.replace(np.nan, 0.0)\n",
    "    df_list.append(tag_df)\n",
    "\n",
    "# Discretize each signal in 3 bins:\n",
    "array = tsia.markov.discretize_multivariate(df_list)\n",
    "\n",
    "# Plot the strip chart:\n",
    "tsia.plot.plot_timeseries_strip_chart(\n",
    "    array, \n",
    "    signal_list=features,\n",
    "    fig_width=24,\n",
    "    signal_height=0.2,\n",
    "    dates=df_list[0].index.to_pydatetime(),\n",
    "    day_interval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00962096",
   "metadata": {},
   "source": [
    "## Preparing the dataset for ingestion\n",
    "---\n",
    "Let's now prepare the data for ingestion into the Amazon Lookout for Equipment service.\n",
    "\n",
    "We need two datasets, the **time series data** and some **label** data: although Lookout for Equipment only uses unsupervised approaches, these label data are used to rank the models trained in the background and select the best one.\n",
    "\n",
    "### Time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df = synth_df[features]\n",
    "timeseries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = os.path.join(PROCESSED_DATA, 'train-data', 'synthetic')\n",
    "os.makedirs(TRAIN_DATA, exist_ok=True)\n",
    "timeseries_fname = os.path.join(TRAIN_DATA, 'sensors.csv')\n",
    "timeseries_df.to_csv(timeseries_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f983944",
   "metadata": {},
   "source": [
    "### Label data\n",
    "We need to transform the label time series into a sequence of time ranges with start time and end time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = synth_df[\n",
    "    (synth_df['machine_status'] == 'RECOVERING') | \n",
    "    (synth_df['machine_status'] == 'BROKEN')\n",
    "].index\n",
    "\n",
    "label_df = pd.DataFrame(index=synth_df.index, columns=['value'])\n",
    "label_df['value'] = 0.0\n",
    "label_df.loc[label_index, 'value'] = 1.0\n",
    "\n",
    "label_df['previous'] = label_df.shift(1, fill_value=0.0)\n",
    "label_df['start']    = (label_df['value'] == 1.0) & (label_df['previous'] == 0.0)\n",
    "label_df['end']      = (label_df['value'] == 0.0) & (label_df['previous'] == 1.0)\n",
    "label_df             = label_df[(label_df['start'] == True) | (label_df['end'] == True)]\n",
    "\n",
    "anomaly_ranges = pd.DataFrame(columns=['start', 'end'])\n",
    "for index, row in label_df.iterrows():\n",
    "    if row['start'] == True:\n",
    "        start = row.name\n",
    "        \n",
    "    if row['end'] == True:\n",
    "        end = row.name\n",
    "        anomaly_ranges = anomaly_ranges.append({'start': start, 'end': end}, ignore_index=True)\n",
    "        \n",
    "anomaly_ranges['start'] = anomaly_ranges['start'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "anomaly_ranges['end'] = anomaly_ranges['end'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "anomaly_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DATA = os.path.join(PROCESSED_DATA, 'label-data')\n",
    "os.makedirs(LABEL_DATA, exist_ok=True)\n",
    "labels_fname = os.path.join(LABEL_DATA, 'synthetic-labels.csv')\n",
    "anomaly_ranges.to_csv(labels_fname, index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f07e40",
   "metadata": {},
   "source": [
    "### Uploading data to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daaa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET       = config.BUCKET\n",
    "TRAIN_PREFIX = config.PREFIX_TRAINING\n",
    "TRAIN_LABEL  = config.PREFIX_LABEL\n",
    "\n",
    "s3_train_prefix = f's3://{BUCKET}/{TRAIN_PREFIX}synthetic/sensors.csv'\n",
    "s3_label_prefix = f's3://{BUCKET}/{TRAIN_LABEL}labels.csv'\n",
    "\n",
    "!aws s3 cp $timeseries_fname $s3_train_prefix\n",
    "!aws s3 cp $labels_fname $s3_label_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6c145",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "In this notebook, you prepared a synthetic dataset for ingestion in Amazon Lookout for Equipment.\n",
    "\n",
    "You also had a quick overview of the dataset with basic timeseries visualization.\n",
    "\n",
    "You uploaded the training time series data and the anomaly labels to Amazon S3: in the next notebook of this getting started, you will be acquainted with the Amazon Lookout for Equipment API to create your first dataset and train a model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
