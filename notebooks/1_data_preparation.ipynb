{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1a9d45",
   "metadata": {},
   "source": [
    "# **Smarter anomaly detection** - Data preparation step\n",
    "*Part 1 - Data preparation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61626092",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". smarter-anomaly-detection\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data are stored here\n",
    "|   ├── processed                        # Finalized datasets ready to be moved to Amazon S3\n",
    "|   └── raw                              # Immutable original data are stored here\n",
    "|\n",
    "└── notebooks/\n",
    "    ├── 1_data_preparation.ipynb         <<< THIS NOTEBOOK <<<\n",
    "    ├── 2_model_training.ipynb\n",
    "    └── 3_model_evaluation.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8e79d",
   "metadata": {},
   "source": [
    "### Notebook configuration update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet --upgrade tqdm tsia kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7776c8e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tsia\n",
    "import zipfile\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d8392",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Let's first check if the bucket name is defined, if it exists and if we have access to it from this notebook. If this notebook does not have access to the S3 bucket, you will have to update its permission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = os.path.join('..', 'data', 'raw')\n",
    "TMP_DATA = os.path.join('..', 'data', 'interim')\n",
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed')\n",
    "os.makedirs(RAW_DATA, exist_ok=True)\n",
    "os.makedirs(TMP_DATA, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA, exist_ok=True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "plt.rcParams['lines.linewidth'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c483e",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "---\n",
    "We are going to use an industrial pump data available from Kaggle. To download this dataset from there, you will need to have an account and create a token that you install on your machine. You can follow [**this link**](https://www.kaggle.com/docs/api) to get started with the Kaggle API. Once generated, make sure your Kaggle token is stored in the `~/.kaggle/kaggle.json` file, or the next cells will issue an error. To get a Kaggle token, go to kaggle.com and create an account. Then click on your profile picture on the top right of the screen, select **Account** and scroll down to the API section. There, click the **Create new API token** button:\n",
    "\n",
    "<img src=\"../assets/kaggle_api.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME    = 'pump-sensor-data.zip'\n",
    "ARCHIVE_PATH = os.path.join(RAW_DATA, FILE_NAME)\n",
    "FILE_PATH    = os.path.join(TMP_DATA, 'pump', 'sensor.csv')\n",
    "FILE_DIR     = os.path.dirname(FILE_PATH)\n",
    "\n",
    "if not os.path.isfile(FILE_PATH):\n",
    "    if not os.path.exists('/home/ec2-user/.kaggle/kaggle.json'):\n",
    "        os.makedirs('/home/ec2-user/.kaggle/', exist_ok=True)\n",
    "        raise Exception('The kaggle.json token was not found.\\nCreating the /home/ec2-user/.kaggle/ directory: put your kaggle.json file there once you have generated it from the Kaggle website')\n",
    "    else:\n",
    "        print('The kaggle.json token file was found: making sure it is not readable by other users on this system.')\n",
    "        !chmod 600 /home/ec2-user/.kaggle/kaggle.json\n",
    "\n",
    "    os.makedirs(os.path.join(TMP_DATA, 'pump'), exist_ok=True)\n",
    "    !kaggle datasets download -d nphantawee/pump-sensor-data -p $RAW_DATA\n",
    "\n",
    "    print(\"\\nExtracting data archive\")\n",
    "    zip_ref = zipfile.ZipFile(ARCHIVE_PATH, 'r')\n",
    "    zip_ref.extractall(FILE_DIR + '/')\n",
    "    zip_ref.close()\n",
    "    print('Done.')\n",
    "    \n",
    "else:\n",
    "    print(\"File found, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d903358",
   "metadata": {},
   "outputs": [],
   "source": [
    "pump_df = pd.read_csv(FILE_PATH, sep=',')\n",
    "pump_df.drop(columns={'Unnamed: 0'}, inplace=True)\n",
    "pump_df['timestamp'] = pd.to_datetime(pump_df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "pump_df = pump_df.set_index('timestamp')\n",
    "\n",
    "print('Shape:', pump_df.shape)\n",
    "print('Original date range:')\n",
    "print(' - from', np.min(pump_df.index), 'to', np.max(pump_df.index))\n",
    "print(' - ', np.max(pump_df.index) - np.min(pump_df.index))\n",
    "pump_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823effcc",
   "metadata": {},
   "source": [
    "Amazon Lookout for Equipment needs at least 3 months of data to train on: this dataset has less than 3 months of data before the last failure present in the dataset. To get more interesting results, we are going to create an artificial time range by keeping the same datapoints but considering they have a 5-minute sampling rate instead of the original 1-minute one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = np.min(pump_df.index)\n",
    "end_date = np.max(pump_df.index)\n",
    "num_periods = pump_df.shape[0]\n",
    "\n",
    "new_index = pd.date_range(end=end_date, periods=num_periods, freq='5min')\n",
    "pump_df.index = new_index\n",
    "pump_df.index.name = 'Timestamp'\n",
    "print('New date range:')\n",
    "print(' - from', np.min(pump_df.index), 'to', np.max(pump_df.index))\n",
    "print(' - ', np.max(pump_df.index) - np.min(pump_df.index))\n",
    "pump_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c95a29",
   "metadata": {},
   "source": [
    "## Dataset visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5454da5",
   "metadata": {},
   "source": [
    "This dataset contains some labels with failure and healing periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_df = pump_df[pump_df['machine_status'] == 'BROKEN'].copy()\n",
    "\n",
    "recovering_df = pd.DataFrame(index=pump_df.index, columns=['value'])\n",
    "recovering_df['value'] = 0.0\n",
    "recovering_index = pump_df[pump_df['machine_status'] == 'RECOVERING'].index\n",
    "recovering_df.loc[recovering_index, 'value'] = 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,4))\n",
    "plt.plot(pump_df['sensor_03'], label='Sensor 03')\n",
    "plt.scatter(broken_df.index, broken_df['sensor_03'], marker='o', color=colors[1], s=100, edgecolor='#000000', alpha=0.8, zorder=3, label='Failure time')\n",
    "plt.fill_between(x=recovering_df.index, y1=recovering_df['value'], color=colors[2], alpha=0.4, label='Recovering period')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Industrial pump sensor data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = list(pump_df.columns)\n",
    "num_cols = 2\n",
    "num_rows = len(tags_list) // num_cols + 1\n",
    "fig = plt.figure(figsize=(24, 5 * num_rows))\n",
    "\n",
    "for index, f in enumerate(tags_list):\n",
    "    ax = fig.add_subplot(num_rows, num_cols, index+1)\n",
    "    ax.plot(pump_df[f], color=colors[index % len(colors)])\n",
    "    ax.set_title(f)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in tags_list if f not in ['sensor_15', 'machine_status']]\n",
    "\n",
    "# Build a list of dataframes, one per feature:\n",
    "df_list = []\n",
    "for sensor in features:\n",
    "    tag_df = pump_df[[sensor]]\n",
    "    tag_df = tag_df.replace(np.nan, 0.0)\n",
    "    df_list.append(tag_df)\n",
    "\n",
    "# Discretize each signal in 3 bins:\n",
    "array = tsia.markov.discretize_multivariate(df_list)\n",
    "\n",
    "# Plot the strip chart:\n",
    "tsia.plot.plot_timeseries_strip_chart(\n",
    "    array, \n",
    "    signal_list=features,\n",
    "    fig_width=24,\n",
    "    signal_height=0.2,\n",
    "    dates=df_list[0].index.to_pydatetime(),\n",
    "    day_interval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567e6fd",
   "metadata": {},
   "source": [
    "## Preparing the dataset for ingestion\n",
    "---\n",
    "Let's now prepare the data for ingestion into the Amazon Lookout for Equipment service.\n",
    "\n",
    "We need two datasets, the **time series data** and some **label** data: although Lookout for Equipment only uses unsupervised approaches, these label data are used to rank the models trained in the background and select the best one.\n",
    "\n",
    "### Time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a475361",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df = pump_df[features]\n",
    "timeseries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26339e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = os.path.join(PROCESSED_DATA, 'train-data', 'pump')\n",
    "os.makedirs(TRAIN_DATA, exist_ok=True)\n",
    "timeseries_fname = os.path.join(TRAIN_DATA, 'sensors.csv')\n",
    "timeseries_df.to_csv(timeseries_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48828dd1",
   "metadata": {},
   "source": [
    "### Label data\n",
    "We need to transform the label time series into a sequence of time ranges with start time and end time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = pump_df[\n",
    "    (pump_df['machine_status'] == 'RECOVERING') | \n",
    "    (pump_df['machine_status'] == 'BROKEN')\n",
    "].index\n",
    "\n",
    "label_df = pd.DataFrame(index=pump_df.index, columns=['value'])\n",
    "label_df['value'] = 0.0\n",
    "label_df.loc[label_index, 'value'] = 1.0\n",
    "\n",
    "label_df['previous'] = label_df.shift(1, fill_value=0.0)\n",
    "label_df['start']    = (label_df['value'] == 1.0) & (label_df['previous'] == 0.0)\n",
    "label_df['end']      = (label_df['value'] == 0.0) & (label_df['previous'] == 1.0)\n",
    "label_df             = label_df[(label_df['start'] == True) | (label_df['end'] == True)]\n",
    "\n",
    "anomaly_ranges = pd.DataFrame(columns=['start', 'end'])\n",
    "for index, row in label_df.iterrows():\n",
    "    if row['start'] == True:\n",
    "        start = row.name\n",
    "        \n",
    "    if row['end'] == True:\n",
    "        end = row.name\n",
    "        anomaly_ranges = anomaly_ranges.append({'start': start, 'end': end}, ignore_index=True)\n",
    "        \n",
    "anomaly_ranges['start'] = anomaly_ranges['start'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "anomaly_ranges['end'] = anomaly_ranges['end'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "anomaly_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e71539",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DATA = os.path.join(PROCESSED_DATA, 'label-data')\n",
    "os.makedirs(LABEL_DATA, exist_ok=True)\n",
    "labels_fname = os.path.join(LABEL_DATA, 'labels.csv')\n",
    "anomaly_ranges.to_csv(labels_fname, index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2234da",
   "metadata": {},
   "source": [
    "### Uploading data to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET       = config.BUCKET\n",
    "TRAIN_PREFIX = config.PREFIX_TRAINING\n",
    "TRAIN_LABEL  = config.PREFIX_LABEL\n",
    "\n",
    "s3_train_prefix = f's3://{BUCKET}/{TRAIN_PREFIX}water-pump/sensors.csv'\n",
    "s3_label_prefix = f's3://{BUCKET}/{TRAIN_LABEL}labels.csv'\n",
    "\n",
    "!aws s3 cp $timeseries_fname $s3_train_prefix\n",
    "!aws s3 cp $labels_fname $s3_label_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34b4cd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "In this notebook, you downloaded an industrial water pump dataset and prepared it for ingestion in Amazon Lookout for Equipment.\n",
    "\n",
    "You also had a quick overview of the dataset with basic timeseries visualization.\n",
    "\n",
    "You uploaded the training time series data and the anomaly labels to Amazon S3: in the next notebook of this getting started, you will be acquainted with the Amazon Lookout for Equipment API to create your first dataset and train a model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
